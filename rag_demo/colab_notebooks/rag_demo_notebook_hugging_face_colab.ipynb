{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vokativ/rag_demo_qb/blob/main/rag_demo/colab_notebooks/rag_demo_notebook_hugging_face_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rnFE4PNve6YO",
      "metadata": {
        "id": "rnFE4PNve6YO"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface-api\n",
        "!pip install chromadb\n",
        "!pip install llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "content-table",
      "metadata": {
        "id": "content-table"
      },
      "source": [
        "# Table of Contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Load Environment Variables](#load-environment-variables)\n",
        "3. [Set up LLM and Embedding Model](#set-up-llm-and-embedding-model)\n",
        "4. [Create In-memory Vector Store](#create-in-memory-vector-store)\n",
        "5. [Create and Load On-disk Vector Store](#create-and-load-on-disk-vector-store)\n",
        "6. [Update and Delete Data](#update-and-delete-data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this basic RAG (Retrieval-Augmented Generation) example, we take a Paul Graham essay, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-environment-variables",
      "metadata": {
        "id": "load-environment-variables"
      },
      "source": [
        "## Load Environment Variables\n",
        "\n",
        "In this section, we will load the necessary environment variables required for accessing the Hugging Face LLMs and Embedding models. <br>\n",
        "\n",
        "Get the Hugging Face Token: <br>\n",
        "How to get your Hugging Face token: https://huggingface.co/docs/hub/en/security-tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "load-env-vars",
      "metadata": {
        "id": "load-env-vars"
      },
      "outputs": [],
      "source": [
        "# Access the Hugging Face environment variables\n",
        "#HF_TOKEN = \"\"\n",
        "\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "set-up-llm-and-embedding-model",
      "metadata": {
        "id": "set-up-llm-and-embedding-model"
      },
      "source": [
        "## Set up LLM and Embedding Model\n",
        "\n",
        "In this section, we will set up the Hugging Face LLM and embedding model using the loaded environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-llm-embedding",
      "metadata": {
        "id": "setup-llm-embedding"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "Settings.llm_model = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-beta\", token=HF_TOKEN)\n",
        "#Settings.llm_model = HuggingFaceInferenceAPI(model_name=\"smirki/UIGEN-T1-Qwen-7b\", token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c70ba4",
      "metadata": {
        "id": "93c70ba4"
      },
      "outputs": [],
      "source": [
        "#Settings.llm_model.complete(\"To infinity, and\")\n",
        "#Settings.llm_model.complete(\"An offer you can not refuse\")\n",
        "Settings.llm_model.complete(\"To be, or not to be\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-documents-and-create-vector-store",
      "metadata": {
        "id": "load-documents-and-create-vector-store"
      },
      "source": [
        "## Create In-memory Vector Store\n",
        "\n",
        "In this section, we will download the document, load the documents, create a Chroma vector store, and store the embedded documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "import-chroma",
      "metadata": {
        "id": "import-chroma"
      },
      "outputs": [],
      "source": [
        "# Import Chroma and other required libraries\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "li5iUGHwaAaf",
      "metadata": {
        "id": "li5iUGHwaAaf"
      },
      "outputs": [],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "#!mkdir -p 'data/shakespeare/'\n",
        "#!mkdir -p 'data/apple_financial/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n",
        "#!wget 'https://www.gutenberg.org/cache/epub/100/pg100.txt' -O 'data/shakespeare/pg100.txt'\n",
        "#!wget 'https://www.apple.com/newsroom/pdfs/fy2025-q1/FY25_Q1_Consolidated_Financial_Statements.pdf' -O 'data/apple_financial/FY25_Q1_Consolidated_Financial_Statements.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "create-vector-store",
      "metadata": {
        "id": "create-vector-store"
      },
      "outputs": [],
      "source": [
        "# Create Chroma client and a new collection\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "#chroma_collection = chroma_client.delete_collection(\"quickstart\")\n",
        "chroma_collection = chroma_client.create_collection(\"quickstart\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Ab8eJIDdbYDc",
      "metadata": {
        "id": "Ab8eJIDdbYDc"
      },
      "outputs": [],
      "source": [
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"/content/data/paul_graham/\").load_data()\n",
        "#documents = SimpleDirectoryReader(\"/content/data/shakespeare/\").load_data()\n",
        "#documents = SimpleDirectoryReader(\"/content/data/apple_financial/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "1pbqz_yzK39m"
      },
      "id": "1pbqz_yzK39m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-vector-store",
      "metadata": {
        "id": "setup-vector-store"
      },
      "outputs": [],
      "source": [
        "# Set up ChromaVectorStore and load in data\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=Settings.embed_model\n",
        ")\n",
        "\n",
        "# Query Data\n",
        "query_engine = index.as_query_engine(llm=Settings.llm_model)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "#response = query_engine.query(\"Sonnet 1\")# times out\n",
        "#response = query_engine.query(\"What was the growth of sales?\") #this will be very wrong\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-and-load-from-disk",
      "metadata": {
        "id": "save-and-load-from-disk"
      },
      "source": [
        "## Create and Load On-disk Vector Store\n",
        "\n",
        "Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to.\n",
        "\n",
        "`Caution`: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other's work. As a best practice, only have one client per path running at any given time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-load-disk",
      "metadata": {
        "id": "save-load-disk"
      },
      "outputs": [],
      "source": [
        "# Save to disk\n",
        "\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=Settings.embed_model\n",
        ")\n",
        "\n",
        "# Load from disk\n",
        "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store,\n",
        "    embed_model=Settings.embed_model,\n",
        ")\n",
        "\n",
        "# Query Data from the persisted index\n",
        "query_engine = index.as_query_engine(llm=Settings.llm_model)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "update-and-delete-data",
      "metadata": {
        "id": "update-and-delete-data"
      },
      "source": [
        "## Update and Delete Data\n",
        "\n",
        "While building toward a real application, you want to go beyond adding data, and also update and delete data.\n",
        "\n",
        "Chroma has users provide `ids` to simplify the bookkeeping here. `ids` can be the name of the file, or a combined hash like `filename_paragraphNumber`, etc.\n",
        "\n",
        "Here is a basic example showing how to do various operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9411826",
      "metadata": {
        "id": "d9411826"
      },
      "outputs": [],
      "source": [
        "doc_to_update = chroma_collection.get(limit=1)\n",
        "doc_to_update[\"metadatas\"][0] = {\n",
        "    **doc_to_update[\"metadatas\"][0],\n",
        "    **{\"author\": \"Paul Graham\"},\n",
        "}\n",
        "chroma_collection.update(\n",
        "    ids=[doc_to_update[\"ids\"][0]], metadatas=[doc_to_update[\"metadatas\"][0]]\n",
        ")\n",
        "updated_doc = chroma_collection.get(limit=1)\n",
        "print(updated_doc[\"metadatas\"][0])\n",
        "\n",
        "# delete the last document\n",
        "print(\"count before\", chroma_collection.count())\n",
        "chroma_collection.delete(ids=[doc_to_update[\"ids\"][0]])\n",
        "print(\"count after\", chroma_collection.count())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "rag-demo-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}